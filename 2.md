# Learning Rate Schedules for Neural Networks

This repository explores various learning rate scheduling techniques for training artificial neural networks (ANNs). Learning rate schedules systematically adjust the learning rate during training to improve model performance and convergence.

## Why Use Learning Rate Schedules?

- **Faster convergence**: Optimal learning rates accelerate training
- **Improved accuracy**: Helps models find better optima
- **Training stability**: Prevents overshooting and oscillations
- **Fine-tuning capability**: Enables precise adjustments in later training stages

## Supported Schedule Types

| Schedule Type          | Description                                                                 |
|------------------------|-----------------------------------------------------------------------------|
| Constant               | Fixed learning rate throughout training                                     |
| Step Decay             | Reduces learning rate by factor after set intervals                         |
| Exponential Decay      | Continuous exponential decrease                                            |
| Time-Based Decay       | Linear reduction over time                                                  |
| Polynomial Decay       | Decreases according to polynomial function                                  |
| Cosine Annealing       | Cyclical variation using cosine function                                   |
| Cyclical Learning Rates| Oscillates between bounds with periodic reset                               |
| Warmup Schedules       | Gradual increase at start, often combined with other schedules             |

